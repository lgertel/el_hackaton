{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Livro de Receitas LangChain üë®‚Äçüç≥üë©‚Äçüç≥"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Este livro de receitas √© baseado na [Documenta√ß√£o Conceitual do LangChain](https://docs.langchain.com/docs/)*\n",
    "\n",
    "**Objetivo:** Fornecer uma compreens√£o introdut√≥ria dos componentes e casos de uso do LangChain com exemplos e trechos de c√≥digo.\n",
    "\n",
    "** Links: **\n",
    "* [Documenta√ß√£o conceitual de LC](https://docs.langchain.com/docs/)\n",
    "* [Documenta√ß√£o do LC Python](https://python.langchain.com/en/latest/)\n",
    "* [LC Javascript/Documenta√ß√£o Typescript](https://js.langchain.com/docs/)\n",
    "* [LC Discord](https://discord.gg/6adMQxSpJS)\n",
    "* [www.langchain.com](https://langchain.com/)\n",
    "* [LC Twitter](https://twitter.com/LangChainAI)\n",
    "\n",
    "\n",
    "### **O que √© LangChain?**\n",
    "> LangChain √© um framework para desenvolvimento de aplica√ß√µes baseadas em modelos de linguagem.\n",
    "\n",
    "**~~TL~~DR**: LangChain facilita as partes complicadas de trabalhar e construir com modelos de IA. Ele ajuda a fazer isso de duas maneiras:\n",
    "\n",
    "1. **Integra√ß√£o** - Traga dados externos, como seus arquivos, outros aplicativos e dados de API, para seus LLMs\n",
    "2. **Ag√™ncia** - Permita que seus LLMs interajam com seu ambiente por meio da tomada de decis√µes. Use LLMs para ajudar a decidir qual a√ß√£o tomar a seguir\n",
    "\n",
    "### **Por que LangChain?**\n",
    "1. **Componentes** - LangChain facilita a troca de abstra√ß√µes e componentes necess√°rios para trabalhar com modelos de linguagem.\n",
    "\n",
    "2. **Correntes Personalizadas** - LangChain fornece suporte pronto para uso e personaliza√ß√£o de 'cadeias' - uma s√©rie de a√ß√µes agrupadas.\n",
    "\n",
    "3. **Velocidade üö¢** - Esta equipe envia incrivelmente r√°pido. Voc√™ estar√° atualizado com os recursos LLM mais recentes.\n",
    "\n",
    "4. **Comunidade üë•** - Discord maravilhoso e suporte da comunidade, encontros, hackathons, etc.\n",
    "\n",
    "Embora os LLMs possam ser diretos (entrada de texto, sa√≠da de texto), voc√™ encontrar√° rapidamente pontos de atrito com os quais o LangChain ajuda quando voc√™ desenvolve aplicativos mais complicados.\n",
    "\n",
    "*Nota: Este livro de receitas n√£o cobrir√° todos os aspectos do LangChain. Seu conte√∫do foi selecionado para que voc√™ crie e impacte o mais r√°pido poss√≠vel. Para saber mais, consulte a [Documenta√ß√£o conceitual do LangChain](https://docs.langchain.com/docs/)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key='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'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Componentes LangChain\n",
    "\n",
    "## Esquema - Detalhes do trabalho com LLMs\n",
    "\n",
    "### **Texto**\n",
    "A maneira de linguagem natural para interagir com LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voc√™ estar√° trabalhando com strings simples (que logo crescer√£o em complexidade!)\n",
    "my_text = \"Que dia vem depois de sexta-feira?\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Mensagens de bate-papo**\n",
    "Como texto, mas especificado com um tipo de mensagem (Sistema, Humano, AI)\n",
    "\n",
    "* **Sistema** - Contexto de segundo plano √∫til que informa √† IA o que fazer\n",
    "* **Humano** - Mensagens que pretendem representar o usu√°rio\n",
    "* **AI** - Mensagens que mostram com o que a IA respondeu\n",
    "\n",
    "Para saber mais, consulte a [documenta√ß√£o] do OpenAI (https://platform.openai.com/docs/guides/chat/introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "chat = ChatOpenAI(temperature=.7, openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Que tal uma salada caprese? √â feita com tomate fresco, mussarela de b√∫fala, manjeric√£o e azeite de oliva. √â uma op√ß√£o saud√°vel e deliciosa!', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(\n",
    "    [\n",
    "    SystemMessage(content=\"Voc√™ √© um bom bot AI que ajuda um usu√°rio a descobrir o que comer em uma frase curta\"),\n",
    "    HumanMessage(content=\"Gosto de tomate, o que devo comer?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voc√™ tamb√©m pode passar mais hist√≥rico de bate-papo com respostas da IA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Em Nice, voc√™ pode visitar a famosa Promenade des Anglais, fazer uma caminhada pelo centro hist√≥rico da cidade, explorar o Museu Matisse e experimentar a deliciosa culin√°ria local. Al√©m disso, voc√™ pode fazer um passeio de barco pela costa ou desfrutar de um dia relaxante na praia.', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(\n",
    "    [\n",
    "        SystemMessage(content=\"Voc√™ √© um bom bot AI que ajuda um usu√°rio a descobrir para onde viajar em uma frase curta\"),\n",
    "        HumanMessage(content=\"Gosto das praias onde devo ir?\"),\n",
    "        AIMessage(content=\"Voc√™ deveria ir para Nice, Fran√ßa\"),\n",
    "        HumanMessage(content=\"O que mais devo fazer quando estiver l√°?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Documentos**\n",
    "Um objeto que cont√©m um peda√ßo de texto e metadados (mais informa√ß√µes sobre esse texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Este √© meu documento. Est√° cheio de texto que reuni de outros lugares', metadata={})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Document(page_content=\"Este √© meu documento. Est√° cheio de texto que reuni de outros lugares\",\n",
    "        metadados={\n",
    "            'my_document_id': 234234,\n",
    "            'my_document_source': \"Os Documentos LangChain\",\n",
    "            'my_document_create_time': 1680013019\n",
    "        })"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models - A interface para os c√©rebros de IA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Modelo de linguagem**\n",
    "Um modelo que faz text in ‚û°Ô∏è text out!\n",
    "\n",
    "*Confira como troquei o modelo que estava usando do padr√£o para o ada-001. Veja mais modelos [aqui](https://platform.openai.com/docs/models)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"text-ada-001\", openai_api_key=openai_api_key)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Modelo de bate-papo**\n",
    "Um modelo que recebe uma s√©rie de mensagens e retorna uma sa√≠da de mensagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "chat = ChatOpenAI(temperature=1, openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Voc√™ poderia tentar andar at√© l√°, mas acho que seria uma caminhada um pouco longa. Talvez seja melhor procurar por op√ß√µes de transporte, como avi√£o ou √¥nibus.', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(\n",
    "    [\n",
    "        SystemMessage(content=\"Voc√™ √© um bot de IA in√∫til que faz piada com tudo o que o usu√°rio diz\"),\n",
    "        HumanMessage(content=\"Gostaria de ir para Nova York, como devo fazer isso?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Modelo de incorpora√ß√£o de texto**\n",
    "Transforme seu texto em um vetor (uma s√©rie de n√∫meros que cont√©m o 'significado' sem√¢ntico do seu texto). Usado principalmente ao comparar dois peda√ßos de texto juntos.\n",
    "\n",
    "*BTW: Sem√¢ntica significa 'relacionado ao significado na linguagem ou na l√≥gica.'*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Ol√°! √â hora da praia\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sua incorpora√ß√£o tem comprimento 1536\n",
      "Aqui est√° um exemplo: [-0.0007985922275111079, 0.001951458165422082, 0.005635906010866165, -0.023960718885064125, -0.008602948859333992]...\n"
     ]
    }
   ],
   "source": [
    "text_embedding = embeddings.embed_query(text)\n",
    "print (f\"Sua incorpora√ß√£o tem comprimento {len(text_embedding)}\")\n",
    "print (f\"Aqui est√° um exemplo: {text_embedding[:5]}...\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompts - Texto geralmente usado como instru√ß√µes para o seu modelo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Prompt**\n",
    "O que voc√™ passar√° para o modelo subjacente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nEssa afirma√ß√£o est√° incorreta pois amanh√£ ser√° ter√ßa-feira, n√£o quarta-feira.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", openai_api_key=openai_api_key)\n",
    "\n",
    "# Eu gosto de usar tr√™s aspas duplas para meus prompts porque √© mais f√°cil de ler\n",
    "prompt = \"\"\"\n",
    "Hoje √© segunda, amanh√£ √© quarta.\n",
    "\n",
    "O que h√° de errado com essa afirma√ß√£o?\n",
    "\"\"\"\n",
    "\n",
    "llm(prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Modelo de Prompt**\n",
    "Um objeto que ajuda a criar prompts com base em uma combina√ß√£o de entrada do usu√°rio, outras informa√ß√µes n√£o est√°ticas e uma sequ√™ncia de modelo fixa.\n",
    "\n",
    "Pense nisso como um [f-string](https://realpython.com/python-f-strings/) em python, mas para prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt final: \n",
      "Eu realmente quero viajar para Rome. O que devo fazer l√°?\n",
      "\n",
      "Responda em uma frase curta\n",
      "\n",
      "-----------\n",
      "Sa√≠da do LLM: Explore os monumentos hist√≥ricos, experimente a culin√°ria local e desfrute da vida noturna.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", openai_api_key=openai_api_key)\n",
    "\n",
    "# Observe \"localiza√ß√£o\" abaixo, que √© um espa√ßo reservado para outro valor posteriormente\n",
    "template = \"\"\"\n",
    "Eu realmente quero viajar para {location}. O que devo fazer l√°?\n",
    "\n",
    "Responda em uma frase curta\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"location\"],\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "final_prompt = prompt.format(location='Rome')\n",
    "\n",
    "print(f\"Prompt final: {final_prompt}\")\n",
    "print(\"-----------\")\n",
    "print(f\"Sa√≠da do LLM: {llm(final_prompt)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exemplos de seletores**\n",
    "Uma maneira f√°cil de selecionar a partir de uma s√©rie de exemplos que permitem que voc√™ coloque informa√ß√µes no contexto de forma din√¢mica em seu prompt. Frequentemente usado quando sua tarefa √© sutil ou voc√™ tem uma grande lista de exemplos.\n",
    "\n",
    "Confira diferentes tipos de seletores de exemplo [aqui](https://python.langchain.com/en/latest/modules/prompts/example_selectors.html)\n",
    "\n",
    "Se voc√™ quiser uma vis√£o geral sobre por que os exemplos s√£o importantes (engenharia de prompt), confira [este v√≠deo](https://www.youtube.com/watch?v=dOxUroR57xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", openai_api_key=openai_api_key)\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Example Input: {input}\\nExample Output: {output}\",\n",
    ")\n",
    "\n",
    "# Exemplos de locais onde os substantivos s√£o encontrados\n",
    "exemplos = [\n",
    "    {\"input\": \"pirata\", \"output\": \"navio\"},\n",
    "    {\"input\": \"piloto\", \"output\": \"avi√£o\"},\n",
    "    {\"input\": \"motorista\", \"output\": \"carro\"},\n",
    "    {\"input\": \"√°rvore\", \"output\": \"solo\"},\n",
    "    {\"input\": \"p√°ssaro\", \"output\": \"ninho\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SemanticSimilarityExampleSelector selecionar√° exemplos que s√£o semelhantes √† sua entrada por significado sem√¢ntico\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    # Esta √© a lista de exemplos dispon√≠veis para sele√ß√£o.\n",
    "    exemplos,\n",
    "\n",
    "    # Esta √© a classe de incorpora√ß√£o usada para produzir incorpora√ß√µes que s√£o usadas para medir a similaridade sem√¢ntica.\n",
    "    OpenAIEmbeddings(openai_api_key=openai_api_key),\n",
    "\n",
    "    # Esta √© a classe VectorStore que √© usada para armazenar os embeddings e fazer uma pesquisa de similaridade.\n",
    "    FAISS,\n",
    "\n",
    "    # Este √© o n√∫mero de exemplos a serem produzidos.\n",
    "    k=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_prompt = FewShotPromptTemplate(\n",
    "    # O objeto que ajudar√° a selecionar exemplos\n",
    "    example_selector=example_selector,\n",
    "\n",
    "    # Seu prompt\n",
    "    example_prompt=example_prompt,\n",
    "\n",
    "    # Personaliza√ß√µes que ser√£o adicionadas √† parte superior e inferior do seu prompt\n",
    "    prefix=\"D√™ a localiza√ß√£o em que um item geralmente √© encontrado\",\n",
    "    suffix=\"Entrada: {noun}\\nSa√≠da:\",\n",
    "\n",
    "    # Quais entradas seu prompt receber√°\n",
    "    input_variables=[\"noun\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D√™ a localiza√ß√£o em que um item geralmente √© encontrado\n",
      "\n",
      "Example Input: motorista\n",
      "Example Output: carro\n",
      "\n",
      "Example Input: piloto\n",
      "Example Output: avi√£o\n",
      "\n",
      "Entrada: estudante\n",
      "Sa√≠da:\n"
     ]
    }
   ],
   "source": [
    "# Selecione um substantivo!\n",
    "meu_substantivo = \"estudante\"\n",
    "\n",
    "print(similar_prompt.format(noun=meu_substantivo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' sala de aula.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(similar_prompt.format(noun=meu_substantivo))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Analisadores de sa√≠da**\n",
    "Uma maneira √∫til de formatar a sa√≠da de um modelo. Geralmente usado para sa√≠da estruturada.\n",
    "\n",
    "Dois grandes conceitos:\n",
    "\n",
    "**1. Instru√ß√µes de formato** - Um prompt gerado automaticamente que informa ao LLM como formatar sua resposta com base no resultado desejado\n",
    "\n",
    "**2. Parser** - Um m√©todo que extrair√° a sa√≠da de texto do seu modelo em uma estrutura desejada (geralmente json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model_name=\"text-davinci-003\", openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructuredOutputParser(response_schemas=[ResponseSchema(name='bad_string', description='Esta √© uma string de entrada de usu√°rio mal formatada'), ResponseSchema(name='good_string', description='Esta √© sua resposta, uma resposta reformatada')])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Como voc√™ gostaria que sua resposta fosse estruturada. Este √© basicamente um modelo de prompt sofisticado\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"bad_string\", description=\"Esta √© uma string de entrada de usu√°rio mal formatada\"),\n",
    "    ResponseSchema(name=\"good_string\", description=\"Esta √© sua resposta, uma resposta reformatada\")\n",
    "]\n",
    "\n",
    "# Como voc√™ gostaria de analisar sua sa√≠da\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"\\`\\`\\`json\" and \"\\`\\`\\`\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"bad_string\": string  // Esta √© uma string de entrada de usu√°rio mal formatada\n",
      "\t\"good_string\": string  // Esta √© sua resposta, uma resposta reformatada\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Veja o modelo de prompt que voc√™ criou para formata√ß√£o\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "print (format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Voc√™ receber√° uma string mal formatada de um usu√°rio.\n",
      "Reformate-o e verifique se todas as palavras est√£o escritas corretamente\n",
      "\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"\\`\\`\\`json\" and \"\\`\\`\\`\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"bad_string\": string  // Esta √© uma string de entrada de usu√°rio mal formatada\n",
      "\t\"good_string\": string  // Esta √© sua resposta, uma resposta reformatada\n",
      "}\n",
      "```\n",
      "\n",
      "% USER INPUT:\n",
      "bem-vindo √† Calif√≥rnya!\n",
      "\n",
      "YOUR RESPONSE:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "Voc√™ receber√° uma string mal formatada de um usu√°rio.\n",
    "Reformate-o e verifique se todas as palavras est√£o escritas corretamente\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "% USER INPUT:\n",
    "{user_input}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"user_input\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    "    template=template\n",
    ")\n",
    "\n",
    "promptValue = prompt.format(user_input=\"bem-vindo √† Calif√≥rnya!\")\n",
    "\n",
    "print(promptValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```json\\n{\\n\\t\"bad_string\": \"bem-vindo √† Calif√≥rnya!\",\\n\\t\"good_string\": \"Bem-vindo √† Calif√≥rnia!\"\\n}\\n```'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_output = llm(promptValue)\n",
    "llm_output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## √çndices - A estrutura√ß√£o de documentos para LLMs pode funcionar com eles"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Carregadores de documentos**\n",
    "Maneiras f√°ceis de importar dados de outras fontes. Funcionalidade compartilhada com [Plugins OpenAI](https://openai.com/blog/chatgpt-plugins) [plugins de recupera√ß√£o espec√≠ficos](https://github.com/openai/chatgpt-retrieval-plugin)\n",
    "\n",
    "Veja uma [grande lista](https://python.langchain.com/en/latest/modules/indexes/document_loaders.html) de carregadores de documentos aqui. Muito mais no [Llama Index](https://llamahub.ai/) tamb√©m."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import HNLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = HNLoader(\"https://news.ycombinator.com/item?id=34422627\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encontrou 76 coment√°rios\n",
      "Aqui est√° um exemplo:\n",
      "\n",
      "Ozzie_osman 4 months ago  \n",
      "             | next [‚Äì] \n",
      "\n",
      "LangChain is awesome. For people not sure what it's doing, large language models (LLMs) are very Ozzie_osman 4 months ago  \n",
      "             | parent | next [‚Äì] \n",
      "\n",
      "Also, another library to check out is GPT Index (https://github.com/jerryjliu/gpt_index)\n"
     ]
    }
   ],
   "source": [
    "print (f\"Encontrou {len(data)} coment√°rios\")\n",
    "print (f\"Aqui est√° um exemplo:\\n\\n{''.join([x.page_content[:150] for x in data[:2]])}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Divisores de texto**\n",
    "Muitas vezes, seu documento √© muito longo (como um livro) para o seu LLM. Voc√™ precisa dividi-lo em peda√ßos. Divisores de texto ajudam com isso.\n",
    "\n",
    "Existem muitas maneiras de dividir seu texto em blocos, experimente [diferentes](https://python.langchain.com/en/latest/modules/indexes/text_splitters.html) para ver qual √© o melhor para voc√™."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voc√™ tem 1 documento\n"
     ]
    }
   ],
   "source": [
    "# Este √© um documento longo que podemos dividir.\n",
    "with open('data/PaulGrahamEssays/worked.txt') as f:\n",
    "    pg_work = f.read()\n",
    "    \n",
    "print (f\"Voc√™ tem {len([pg_work])} documento\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "     # Defina um tamanho de bloco realmente pequeno, apenas para mostrar.\n",
    "     chunk_size = 150,\n",
    "     chunk_overlap = 20,\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents([pg_work])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizar:\n",
      "February 2021Before college the two main things I worked on, outside of school,\n",
      "were writing and programming. I didn't write essays. I wrote what \n",
      "\n",
      "beginning writers were supposed to write then, and probably still\n",
      "are: short stories. My stories were awful. They had hardly any plot,\n"
     ]
    }
   ],
   "source": [
    "print (\"Visualizar:\")\n",
    "print (texts[0].page_content, \"\\n\")\n",
    "print (texts[1].page_content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Recuperadores**\n",
    "Maneira f√°cil de combinar documentos com modelos de idioma.\n",
    "\n",
    "Existem muitos tipos diferentes de recuperadores, o mais amplamente suportado √© o VectoreStoreRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "loader = TextLoader('data/PaulGrahamEssays/worked.txt')\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare seu divisor\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "\n",
    "# Divida seus documentos em textos\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# Prepare o mecanismo de incorpora√ß√£o\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "\n",
    "# Incorpore seus textos\n",
    "db = FAISS.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retriever.get_relevant_documents(\"que tipos de coisas o autor queria construir?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standards; what was the point? No one else wanted one either, so\n",
      "off they went. That was what happened to systems work.I wanted not just to build things, but to build things that would\n",
      "last.In this di\n",
      "\n",
      "hackers dream of building a new Lisp, partly because one of the\n",
      "distinctive features of the language is that it has dialects, and\n",
      "partly, I think, because we have in our minds a Platonic form of\n",
      "Lisp \n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n\".join([x.page_content[:200] for x in docs[:2]]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correntes ‚õìÔ∏è‚õìÔ∏è‚õìÔ∏è\n",
    "Combinando diferentes chamadas e a√ß√µes LLM automaticamente\n",
    "\n",
    "Ex: Resumo #1, Resumo #2, Resumo #3 > Resumo Final\n",
    "\n",
    "Confira [este v√≠deo](https://www.youtube.com/watch?v=f9_BWhCI4Zo&t=2s) explicando diferentes tipos de cadeia de resumo\n",
    "\n",
    "Existem [muitas aplica√ß√µes de chains](https://python.langchain.com/en/latest/modules/chains/how_to_guides.html) pesquisa para ver quais s√£o as melhores para o seu caso de uso.\n",
    "\n",
    "Abordaremos dois deles:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Cadeias Sequenciais Simples\n",
    "\n",
    "Cadeias f√°ceis onde voc√™ pode usar a sa√≠da de um LLM como entrada em outro. Bom para dividir tarefas (e manter seu LLM focado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "llm = OpenAI(temperature=1, openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Seu trabalho √© inventar um prato cl√°ssico da √°rea que os usu√°rios sugerem.\n",
    "% USER LOCATION\n",
    "{user_location}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"user_location\"], template=template)\n",
    "\n",
    "# Mant√©m minha cadeia de 'localiza√ß√£o'\n",
    "location_chain = LLMChain(llm=llm, prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Dada uma refei√ß√£o, d√™ uma receita curta e simples de como fazer esse prato em casa.\n",
    "% MEAL\n",
    "{user_meal}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"user_meal\"], template=template)\n",
    "\n",
    "# Mant√©m minha cadeia de 'refei√ß√£o'\n",
    "meal_chain = LLMChain(llm=llm, prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_chain = SimpleSequentialChain(chains=[location_chain, meal_chain], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mO Ravioli alla Carbonara di Roma √© um prato tradicional romano. √â feito com massa de ravioli caseiro e preenchido com molho de bacon, gemas de ovo, parmes√£o e pimenta. O prato √© um prato t√≠pico do Lazio, mas √© extremamente popular em toda a It√°lia.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m\n",
      "Ravioli alla Carbonara di Roma\n",
      "\n",
      "Ingredientes:\n",
      "- 500g de farinha de trigo\n",
      "- 4 ovos\n",
      "- 200g de bacon picado\n",
      "- 200g de queijo parmes√£o ralado\n",
      "- Sal e pimenta a gosto\n",
      "\n",
      "Instructions:\n",
      "\n",
      "1. Misture a farinha e os ovos at√© obter uma massa homog√™nea.\n",
      "2. Coloque a massa em uma superf√≠cie lisa e abra-a com um rolo at√© ficar com cerca de 3mm de espessura.\n",
      "3. Corte c√≠rculos de 8-10 cm usando um cortador de biscoitos.\n",
      "4. Coloque um pouco de bacon picado em cada c√≠rculo e dobre em dois.\n",
      "5. Pressione o bordo da massa com os dedos para fechar os ravioli.\n",
      "6. Cozinhe os ravioli em √°gua fervente por cerca de 4-5 minutos.\n",
      "7. Escorra\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "review = overall_chain.run(\"Roma\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Cadeia de resumo\n",
    "\n",
    "Percorra facilmente v√°rios documentos longos e obtenha um resumo. Confira [este v√≠deo](https://www.youtube.com/watch?v=f9_BWhCI4Zo) para outros tipos de correntes al√©m de map-reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"January 2017Because biographies of famous scientists tend to \n",
      "edit out their mistakes, we underestimate the \n",
      "degree of risk they were willing to take.\n",
      "And because anything a famous scientist did that\n",
      "wasn't a mistake has probably now become the\n",
      "conventional wisdom, those choices don't\n",
      "seem risky either.Biographies of Newton, for example, understandably focus\n",
      "more on physics than alchemy or theology.\n",
      "The impression we get is that his unerring judgment\n",
      "led him straight to truths no one else had noticed.\n",
      "How to explain all the time he spent on alchemy\n",
      "and theology?  Well, smart people are often kind of\n",
      "crazy.But maybe there is a simpler explanation. Maybe\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"the smartness and the craziness were not as separate\n",
      "as we think. Physics seems to us a promising thing\n",
      "to work on, and alchemy and theology obvious wastes\n",
      "of time. But that's because we know how things\n",
      "turned out. In Newton's day the three problems \n",
      "seemed roughly equally promising. No one knew yet\n",
      "what the payoff would be for inventing what we\n",
      "now call physics; if they had, more people would \n",
      "have been working on it. And alchemy and theology\n",
      "were still then in the category Marc Andreessen would \n",
      "describe as \"huge, if true.\"Newton made three bets. One of them worked. But \n",
      "they were all risky.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\" Biographies of famous scientists tend to highlight their accomplishments while excluding mistakes, leading people to overlook the risks they took. Newton's experiments on alchemy and theology often go ignored or underestimated, making his choices look like unerring judgment when in fact, they could have been risky. Maybe there is an alternate explanation.\n",
      "\n",
      " Newton made three bets‚Äîphysics, alchemy and theology‚Äîand only one ultimately paid off. However, all three seemed to be equally promising in Newton's time since the payout of inventing what is now physics was still unknown. Therefore, the \"smartness\" and \"craziness\" of his bets were intermixed.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Biographies of famous scientists frequently ignore or downplay their risks by emphasizing solely their achievements, leaving the impression that their decisions were error-free. According to this article, Isaac Newton was apparently quite daring; although only one of his bets--physics--ultimately paid off, all three were of equal value at the time he took them since the merit of physics was not yet known. His choices were a combination of \"smartness\" and \"craziness\", reflecting the risk inherent in any innovator\\'s venture.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = TextLoader('data/PaulGrahamEssays/disc.txt')\n",
    "documents = loader.load()\n",
    "\n",
    "# Prepare seu divisor\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=50)\n",
    "\n",
    "# Divida seus documentos em textos\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# H√° muita complexidade escondida nesta linha. Eu encorajo voc√™ a conferir o v√≠deo acima para mais detalhes\n",
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\", verbose=True)\n",
    "chain.run(texts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agentes ü§ñü§ñ\n",
    "\n",
    "A documenta√ß√£o oficial da LangChain descreve os agentes perfeitamente (√™nfase minha):\n",
    "> Alguns aplicativos exigir√£o n√£o apenas uma cadeia predeterminada de chamadas para LLMs/outras ferramentas, mas potencialmente uma **cadeia desconhecida** que depende da entrada do usu√°rio. Nesses tipos de cadeias, existe um ‚Äúagente‚Äù que tem acesso a um conjunto de ferramentas. Dependendo da entrada do usu√°rio, o agente pode ent√£o **decidir qual dessas ferramentas chamar**.\n",
    "\n",
    "\n",
    "Basicamente, voc√™ usa o LLM n√£o apenas para sa√≠da de texto, mas tamb√©m para tomada de decis√£o. A frieza e o poder dessa funcionalidade n√£o podem ser exagerados o suficiente.\n",
    "\n",
    "Sam Altman enfatiza que os LLMs s√£o bons '[mecanismo de racioc√≠nio](https://www.youtube.com/watch?v=L_Guz73e6fw&t=867s)'. Agente tire proveito disso."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agentes\n",
    "\n",
    "O modelo de linguagem que impulsiona a tomada de decis√£o.\n",
    "\n",
    "Mais especificamente, um agente recebe uma entrada e retorna uma resposta correspondente a uma a√ß√£o a ser executada junto com uma entrada de a√ß√£o. Voc√™ pode ver diferentes tipos de agentes (que s√£o melhores para diferentes casos de uso) [aqui](https://python.langchain.com/en/latest/modules/agents/agents/agent_types.html)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ferramentas\n",
    "\n",
    "Uma 'capacidade' de um agente. Esta √© uma abstra√ß√£o em cima de uma fun√ß√£o que facilita a intera√ß√£o dos LLMs (e agentes) com ela. Exemplo: pesquisa no Google.\n",
    "\n",
    "Esta √°rea compartilha pontos em comum com [plug-ins OpenAI](https://platform.openai.com/docs/plugins/introduction)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conjunto de ferramentas\n",
    "\n",
    "Grupos de ferramentas que seu agente pode selecionar\n",
    "\n",
    "Vamos junt√°-los:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.llms import OpenAI\n",
    "import json\n",
    "\n",
    "llm = OpenAI(temperature=0, openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "serpapi_api_key='5facac9be1461ad40835cb370aed6c901b4361a81a7dd4ef940ac6f6e6ace6c0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkit = load_tools([\"serpapi\"], llm=llm, serpapi_api_key=serpapi_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = initialize_agent(toolkit, llm, agent=\"zero-shot-react-description\", verbose=True, return_intermediate_steps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should research the band and Natalie Bergman\n",
      "Action: Search\n",
      "Action Input: Natalie Bergman band\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mNatalie Bergman is an American singer-songwriter. She is one half of the duo Wild Belle, along with her brother Elliot Bergman. Her debut solo album, Mercy, was released on Third Man Records on May 7, 2021. She is based in Los Angeles.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should look for the name of the first album\n",
      "Action: Search\n",
      "Action Input: Wild Belle first album\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mIsles\u001b[0m\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: Isles\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response = agent({\"input\":\"qual foi o primeiro √°lbum da\"\n",
    "                    \"banda da qual Natalie Bergman faz parte?\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "el_teste",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

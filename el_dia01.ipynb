{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Livro de Receitas LangChain ðŸ‘¨â€ðŸ³ðŸ‘©â€ðŸ³"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Este livro de receitas Ã© baseado na [DocumentaÃ§Ã£o Conceitual do LangChain](https://docs.langchain.com/docs/)*\n",
    "\n",
    "**Objetivo:** Fornecer uma compreensÃ£o introdutÃ³ria dos componentes e casos de uso do LangChain com exemplos e trechos de cÃ³digo.\n",
    "\n",
    "** Links: **\n",
    "* [DocumentaÃ§Ã£o conceitual de LC](https://docs.langchain.com/docs/)\n",
    "* [DocumentaÃ§Ã£o do LC Python](https://python.langchain.com/en/latest/)\n",
    "* [LC Javascript/DocumentaÃ§Ã£o Typescript](https://js.langchain.com/docs/)\n",
    "* [LC Discord](https://discord.gg/6adMQxSpJS)\n",
    "* [www.langchain.com](https://langchain.com/)\n",
    "* [LC Twitter](https://twitter.com/LangChainAI)\n",
    "\n",
    "\n",
    "### **O que Ã© LangChain?**\n",
    "> LangChain Ã© um framework para desenvolvimento de aplicaÃ§Ãµes baseadas em modelos de linguagem.\n",
    "\n",
    "**~~TL~~DR**: LangChain facilita as partes complicadas de trabalhar e construir com modelos de IA. Ele ajuda a fazer isso de duas maneiras:\n",
    "\n",
    "1. **IntegraÃ§Ã£o** - Traga dados externos, como seus arquivos, outros aplicativos e dados de API, para seus LLMs\n",
    "2. **AgÃªncia** - Permita que seus LLMs interajam com seu ambiente por meio da tomada de decisÃµes. Use LLMs para ajudar a decidir qual aÃ§Ã£o tomar a seguir\n",
    "\n",
    "### **Por que LangChain?**\n",
    "1. **Componentes** - LangChain facilita a troca de abstraÃ§Ãµes e componentes necessÃ¡rios para trabalhar com modelos de linguagem.\n",
    "\n",
    "2. **Correntes Personalizadas** - LangChain fornece suporte pronto para uso e personalizaÃ§Ã£o de 'cadeias' - uma sÃ©rie de aÃ§Ãµes agrupadas.\n",
    "\n",
    "3. **Velocidade ðŸš¢** - Esta equipe envia incrivelmente rÃ¡pido. VocÃª estarÃ¡ atualizado com os recursos LLM mais recentes.\n",
    "\n",
    "4. **Comunidade ðŸ‘¥** - Discord maravilhoso e suporte da comunidade, encontros, hackathons, etc.\n",
    "\n",
    "Embora os LLMs possam ser diretos (entrada de texto, saÃ­da de texto), vocÃª encontrarÃ¡ rapidamente pontos de atrito com os quais o LangChain ajuda quando vocÃª desenvolve aplicativos mais complicados.\n",
    "\n",
    "*Nota: Este livro de receitas nÃ£o cobrirÃ¡ todos os aspectos do LangChain. Seu conteÃºdo foi selecionado para que vocÃª crie e impacte o mais rÃ¡pido possÃ­vel. Para saber mais, consulte a [DocumentaÃ§Ã£o conceitual do LangChain](https://docs.langchain.com/docs/)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key='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'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Componentes LangChain\n",
    "\n",
    "## Esquema - Detalhes do trabalho com LLMs\n",
    "\n",
    "### **Texto**\n",
    "A maneira de linguagem natural para interagir com LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VocÃª estarÃ¡ trabalhando com strings simples (que logo crescerÃ£o em complexidade!)\n",
    "my_text = \"Que dia vem depois de sexta-feira?\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Mensagens de bate-papo**\n",
    "Como texto, mas especificado com um tipo de mensagem (Sistema, Humano, AI)\n",
    "\n",
    "* **Sistema** - Contexto de segundo plano Ãºtil que informa Ã  IA o que fazer\n",
    "* **Humano** - Mensagens que pretendem representar o usuÃ¡rio\n",
    "* **AI** - Mensagens que mostram com o que a IA respondeu\n",
    "\n",
    "Para saber mais, consulte a [documentaÃ§Ã£o] do OpenAI (https://platform.openai.com/docs/guides/chat/introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "chat = ChatOpenAI(temperature=.7, openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Que tal uma salada caprese? Ã‰ feita com tomate fresco, mussarela de bÃºfala, manjericÃ£o e azeite de oliva. Ã‰ uma opÃ§Ã£o saudÃ¡vel e deliciosa!', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(\n",
    "    [\n",
    "    SystemMessage(content=\"VocÃª Ã© um bom bot AI que ajuda um usuÃ¡rio a descobrir o que comer em uma frase curta\"),\n",
    "    HumanMessage(content=\"Gosto de tomate, o que devo comer?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VocÃª tambÃ©m pode passar mais histÃ³rico de bate-papo com respostas da IA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Em Nice, vocÃª pode visitar a famosa Promenade des Anglais, fazer uma caminhada pelo centro histÃ³rico da cidade, explorar o Museu Matisse e experimentar a deliciosa culinÃ¡ria local. AlÃ©m disso, vocÃª pode fazer um passeio de barco pela costa ou desfrutar de um dia relaxante na praia.', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(\n",
    "    [\n",
    "        SystemMessage(content=\"VocÃª Ã© um bom bot AI que ajuda um usuÃ¡rio a descobrir para onde viajar em uma frase curta\"),\n",
    "        HumanMessage(content=\"Gosto das praias onde devo ir?\"),\n",
    "        AIMessage(content=\"VocÃª deveria ir para Nice, FranÃ§a\"),\n",
    "        HumanMessage(content=\"O que mais devo fazer quando estiver lÃ¡?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Documentos**\n",
    "Um objeto que contÃ©m um pedaÃ§o de texto e metadados (mais informaÃ§Ãµes sobre esse texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Este Ã© meu documento. EstÃ¡ cheio de texto que reuni de outros lugares', metadata={})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Document(page_content=\"Este Ã© meu documento. EstÃ¡ cheio de texto que reuni de outros lugares\",\n",
    "        metadados={\n",
    "            'my_document_id': 234234,\n",
    "            'my_document_source': \"Os Documentos LangChain\",\n",
    "            'my_document_create_time': 1680013019\n",
    "        })"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models - A interface para os cÃ©rebros de IA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Modelo de linguagem**\n",
    "Um modelo que faz text in âž¡ï¸ text out!\n",
    "\n",
    "*Confira como troquei o modelo que estava usando do padrÃ£o para o ada-001. Veja mais modelos [aqui](https://platform.openai.com/docs/models)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"text-ada-001\", openai_api_key=openai_api_key)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Modelo de bate-papo**\n",
    "Um modelo que recebe uma sÃ©rie de mensagens e retorna uma saÃ­da de mensagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "chat = ChatOpenAI(temperature=1, openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='VocÃª poderia tentar andar atÃ© lÃ¡, mas acho que seria uma caminhada um pouco longa. Talvez seja melhor procurar por opÃ§Ãµes de transporte, como aviÃ£o ou Ã´nibus.', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(\n",
    "    [\n",
    "        SystemMessage(content=\"VocÃª Ã© um bot de IA inÃºtil que faz piada com tudo o que o usuÃ¡rio diz\"),\n",
    "        HumanMessage(content=\"Gostaria de ir para Nova York, como devo fazer isso?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Modelo de incorporaÃ§Ã£o de texto**\n",
    "Transforme seu texto em um vetor (uma sÃ©rie de nÃºmeros que contÃ©m o 'significado' semÃ¢ntico do seu texto). Usado principalmente ao comparar dois pedaÃ§os de texto juntos.\n",
    "\n",
    "*BTW: SemÃ¢ntica significa 'relacionado ao significado na linguagem ou na lÃ³gica.'*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"OlÃ¡! Ã‰ hora da praia\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sua incorporaÃ§Ã£o tem comprimento 1536\n",
      "Aqui estÃ¡ um exemplo: [-0.0007985922275111079, 0.001951458165422082, 0.005635906010866165, -0.023960718885064125, -0.008602948859333992]...\n"
     ]
    }
   ],
   "source": [
    "text_embedding = embeddings.embed_query(text)\n",
    "print (f\"Sua incorporaÃ§Ã£o tem comprimento {len(text_embedding)}\")\n",
    "print (f\"Aqui estÃ¡ um exemplo: {text_embedding[:5]}...\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompts - Texto geralmente usado como instruÃ§Ãµes para o seu modelo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Prompt**\n",
    "O que vocÃª passarÃ¡ para o modelo subjacente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nEssa afirmaÃ§Ã£o estÃ¡ incorreta pois amanhÃ£ serÃ¡ terÃ§a-feira, nÃ£o quarta-feira.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", openai_api_key=openai_api_key)\n",
    "\n",
    "# Eu gosto de usar trÃªs aspas duplas para meus prompts porque Ã© mais fÃ¡cil de ler\n",
    "prompt = \"\"\"\n",
    "Hoje Ã© segunda, amanhÃ£ Ã© quarta.\n",
    "\n",
    "O que hÃ¡ de errado com essa afirmaÃ§Ã£o?\n",
    "\"\"\"\n",
    "\n",
    "llm(prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Modelo de Prompt**\n",
    "Um objeto que ajuda a criar prompts com base em uma combinaÃ§Ã£o de entrada do usuÃ¡rio, outras informaÃ§Ãµes nÃ£o estÃ¡ticas e uma sequÃªncia de modelo fixa.\n",
    "\n",
    "Pense nisso como um [f-string](https://realpython.com/python-f-strings/) em python, mas para prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt final: \n",
      "Eu realmente quero viajar para Rome. O que devo fazer lÃ¡?\n",
      "\n",
      "Responda em uma frase curta\n",
      "\n",
      "-----------\n",
      "SaÃ­da do LLM: Explore os monumentos histÃ³ricos, experimente a culinÃ¡ria local e desfrute da vida noturna.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", openai_api_key=openai_api_key)\n",
    "\n",
    "# Observe \"localizaÃ§Ã£o\" abaixo, que Ã© um espaÃ§o reservado para outro valor posteriormente\n",
    "template = \"\"\"\n",
    "Eu realmente quero viajar para {location}. O que devo fazer lÃ¡?\n",
    "\n",
    "Responda em uma frase curta\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"location\"],\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "final_prompt = prompt.format(location='Rome')\n",
    "\n",
    "print(f\"Prompt final: {final_prompt}\")\n",
    "print(\"-----------\")\n",
    "print(f\"SaÃ­da do LLM: {llm(final_prompt)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exemplos de seletores**\n",
    "Uma maneira fÃ¡cil de selecionar a partir de uma sÃ©rie de exemplos que permitem que vocÃª coloque informaÃ§Ãµes no contexto de forma dinÃ¢mica em seu prompt. Frequentemente usado quando sua tarefa Ã© sutil ou vocÃª tem uma grande lista de exemplos.\n",
    "\n",
    "Confira diferentes tipos de seletores de exemplo [aqui](https://python.langchain.com/en/latest/modules/prompts/example_selectors.html)\n",
    "\n",
    "Se vocÃª quiser uma visÃ£o geral sobre por que os exemplos sÃ£o importantes (engenharia de prompt), confira [este vÃ­deo](https://www.youtube.com/watch?v=dOxUroR57xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", openai_api_key=openai_api_key)\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Example Input: {input}\\nExample Output: {output}\",\n",
    ")\n",
    "\n",
    "# Exemplos de locais onde os substantivos sÃ£o encontrados\n",
    "exemplos = [\n",
    "    {\"input\": \"pirata\", \"output\": \"navio\"},\n",
    "    {\"input\": \"piloto\", \"output\": \"aviÃ£o\"},\n",
    "    {\"input\": \"motorista\", \"output\": \"carro\"},\n",
    "    {\"input\": \"Ã¡rvore\", \"output\": \"solo\"},\n",
    "    {\"input\": \"pÃ¡ssaro\", \"output\": \"ninho\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SemanticSimilarityExampleSelector selecionarÃ¡ exemplos que sÃ£o semelhantes Ã  sua entrada por significado semÃ¢ntico\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    # Esta Ã© a lista de exemplos disponÃ­veis para seleÃ§Ã£o.\n",
    "    exemplos,\n",
    "\n",
    "    # Esta Ã© a classe de incorporaÃ§Ã£o usada para produzir incorporaÃ§Ãµes que sÃ£o usadas para medir a similaridade semÃ¢ntica.\n",
    "    OpenAIEmbeddings(openai_api_key=openai_api_key),\n",
    "\n",
    "    # Esta Ã© a classe VectorStore que Ã© usada para armazenar os embeddings e fazer uma pesquisa de similaridade.\n",
    "    FAISS,\n",
    "\n",
    "    # Este Ã© o nÃºmero de exemplos a serem produzidos.\n",
    "    k=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_prompt = FewShotPromptTemplate(\n",
    "    # O objeto que ajudarÃ¡ a selecionar exemplos\n",
    "    example_selector=example_selector,\n",
    "\n",
    "    # Seu prompt\n",
    "    example_prompt=example_prompt,\n",
    "\n",
    "    # PersonalizaÃ§Ãµes que serÃ£o adicionadas Ã  parte superior e inferior do seu prompt\n",
    "    prefix=\"DÃª a localizaÃ§Ã£o em que um item geralmente Ã© encontrado\",\n",
    "    suffix=\"Entrada: {noun}\\nSaÃ­da:\",\n",
    "\n",
    "    # Quais entradas seu prompt receberÃ¡\n",
    "    input_variables=[\"noun\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DÃª a localizaÃ§Ã£o em que um item geralmente Ã© encontrado\n",
      "\n",
      "Example Input: motorista\n",
      "Example Output: carro\n",
      "\n",
      "Example Input: piloto\n",
      "Example Output: aviÃ£o\n",
      "\n",
      "Entrada: estudante\n",
      "SaÃ­da:\n"
     ]
    }
   ],
   "source": [
    "# Selecione um substantivo!\n",
    "meu_substantivo = \"estudante\"\n",
    "\n",
    "print(similar_prompt.format(noun=meu_substantivo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' sala de aula.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(similar_prompt.format(noun=meu_substantivo))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Analisadores de saÃ­da**\n",
    "Uma maneira Ãºtil de formatar a saÃ­da de um modelo. Geralmente usado para saÃ­da estruturada.\n",
    "\n",
    "Dois grandes conceitos:\n",
    "\n",
    "**1. InstruÃ§Ãµes de formato** - Um prompt gerado automaticamente que informa ao LLM como formatar sua resposta com base no resultado desejado\n",
    "\n",
    "**2. Parser** - Um mÃ©todo que extrairÃ¡ a saÃ­da de texto do seu modelo em uma estrutura desejada (geralmente json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model_name=\"text-davinci-003\", openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructuredOutputParser(response_schemas=[ResponseSchema(name='bad_string', description='Esta Ã© uma string de entrada de usuÃ¡rio mal formatada'), ResponseSchema(name='good_string', description='Esta Ã© sua resposta, uma resposta reformatada')])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Como vocÃª gostaria que sua resposta fosse estruturada. Este Ã© basicamente um modelo de prompt sofisticado\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"bad_string\", description=\"Esta Ã© uma string de entrada de usuÃ¡rio mal formatada\"),\n",
    "    ResponseSchema(name=\"good_string\", description=\"Esta Ã© sua resposta, uma resposta reformatada\")\n",
    "]\n",
    "\n",
    "# Como vocÃª gostaria de analisar sua saÃ­da\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"\\`\\`\\`json\" and \"\\`\\`\\`\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"bad_string\": string  // Esta Ã© uma string de entrada de usuÃ¡rio mal formatada\n",
      "\t\"good_string\": string  // Esta Ã© sua resposta, uma resposta reformatada\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Veja o modelo de prompt que vocÃª criou para formataÃ§Ã£o\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "print (format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VocÃª receberÃ¡ uma string mal formatada de um usuÃ¡rio.\n",
      "Reformate-o e verifique se todas as palavras estÃ£o escritas corretamente\n",
      "\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"\\`\\`\\`json\" and \"\\`\\`\\`\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"bad_string\": string  // Esta Ã© uma string de entrada de usuÃ¡rio mal formatada\n",
      "\t\"good_string\": string  // Esta Ã© sua resposta, uma resposta reformatada\n",
      "}\n",
      "```\n",
      "\n",
      "% USER INPUT:\n",
      "bem-vindo Ã  CalifÃ³rnya!\n",
      "\n",
      "YOUR RESPONSE:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "VocÃª receberÃ¡ uma string mal formatada de um usuÃ¡rio.\n",
    "Reformate-o e verifique se todas as palavras estÃ£o escritas corretamente\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "% USER INPUT:\n",
    "{user_input}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"user_input\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    "    template=template\n",
    ")\n",
    "\n",
    "promptValue = prompt.format(user_input=\"bem-vindo Ã  CalifÃ³rnya!\")\n",
    "\n",
    "print(promptValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```json\\n{\\n\\t\"bad_string\": \"bem-vindo Ã  CalifÃ³rnya!\",\\n\\t\"good_string\": \"Bem-vindo Ã  CalifÃ³rnia!\"\\n}\\n```'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_output = llm(promptValue)\n",
    "llm_output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ãndices - A estruturaÃ§Ã£o de documentos para LLMs pode funcionar com eles"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Carregadores de documentos**\n",
    "Maneiras fÃ¡ceis de importar dados de outras fontes. Funcionalidade compartilhada com [Plugins OpenAI](https://openai.com/blog/chatgpt-plugins) [plugins de recuperaÃ§Ã£o especÃ­ficos](https://github.com/openai/chatgpt-retrieval-plugin)\n",
    "\n",
    "Veja uma [grande lista](https://python.langchain.com/en/latest/modules/indexes/document_loaders.html) de carregadores de documentos aqui. Muito mais no [Llama Index](https://llamahub.ai/) tambÃ©m."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import HNLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = HNLoader(\"https://news.ycombinator.com/item?id=34422627\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encontrou 76 comentÃ¡rios\n",
      "Aqui estÃ¡ um exemplo:\n",
      "\n",
      "Ozzie_osman 4 months ago  \n",
      "             | next [â€“] \n",
      "\n",
      "LangChain is awesome. For people not sure what it's doing, large language models (LLMs) are very Ozzie_osman 4 months ago  \n",
      "             | parent | next [â€“] \n",
      "\n",
      "Also, another library to check out is GPT Index (https://github.com/jerryjliu/gpt_index)\n"
     ]
    }
   ],
   "source": [
    "print (f\"Encontrou {len(data)} comentÃ¡rios\")\n",
    "print (f\"Aqui estÃ¡ um exemplo:\\n\\n{''.join([x.page_content[:150] for x in data[:2]])}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Divisores de texto**\n",
    "Muitas vezes, seu documento Ã© muito longo (como um livro) para o seu LLM. VocÃª precisa dividi-lo em pedaÃ§os. Divisores de texto ajudam com isso.\n",
    "\n",
    "Existem muitas maneiras de dividir seu texto em blocos, experimente [diferentes](https://python.langchain.com/en/latest/modules/indexes/text_splitters.html) para ver qual Ã© o melhor para vocÃª."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VocÃª tem 1 documento\n"
     ]
    }
   ],
   "source": [
    "# Este Ã© um documento longo que podemos dividir.\n",
    "with open('data/PaulGrahamEssays/worked.txt') as f:\n",
    "    pg_work = f.read()\n",
    "    \n",
    "print (f\"VocÃª tem {len([pg_work])} documento\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "     # Defina um tamanho de bloco realmente pequeno, apenas para mostrar.\n",
    "     chunk_size = 150,\n",
    "     chunk_overlap = 20,\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents([pg_work])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizar:\n",
      "February 2021Before college the two main things I worked on, outside of school,\n",
      "were writing and programming. I didn't write essays. I wrote what \n",
      "\n",
      "beginning writers were supposed to write then, and probably still\n",
      "are: short stories. My stories were awful. They had hardly any plot,\n"
     ]
    }
   ],
   "source": [
    "print (\"Visualizar:\")\n",
    "print (texts[0].page_content, \"\\n\")\n",
    "print (texts[1].page_content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Recuperadores**\n",
    "Maneira fÃ¡cil de combinar documentos com modelos de idioma.\n",
    "\n",
    "Existem muitos tipos diferentes de recuperadores, o mais amplamente suportado Ã© o VectoreStoreRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "loader = TextLoader('data/PaulGrahamEssays/worked.txt')\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare seu divisor\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "\n",
    "# Divida seus documentos em textos\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# Prepare o mecanismo de incorporaÃ§Ã£o\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "\n",
    "# Incorpore seus textos\n",
    "db = FAISS.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retriever.get_relevant_documents(\"que tipos de coisas o autor queria construir?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standards; what was the point? No one else wanted one either, so\n",
      "off they went. That was what happened to systems work.I wanted not just to build things, but to build things that would\n",
      "last.In this di\n",
      "\n",
      "hackers dream of building a new Lisp, partly because one of the\n",
      "distinctive features of the language is that it has dialects, and\n",
      "partly, I think, because we have in our minds a Platonic form of\n",
      "Lisp \n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n\".join([x.page_content[:200] for x in docs[:2]]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correntes â›“ï¸â›“ï¸â›“ï¸\n",
    "Combinando diferentes chamadas e aÃ§Ãµes LLM automaticamente\n",
    "\n",
    "Ex: Resumo #1, Resumo #2, Resumo #3 > Resumo Final\n",
    "\n",
    "Confira [este vÃ­deo](https://www.youtube.com/watch?v=f9_BWhCI4Zo&t=2s) explicando diferentes tipos de cadeia de resumo\n",
    "\n",
    "Existem [muitas aplicaÃ§Ãµes de chains](https://python.langchain.com/en/latest/modules/chains/how_to_guides.html) pesquisa para ver quais sÃ£o as melhores para o seu caso de uso.\n",
    "\n",
    "Abordaremos dois deles:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Cadeias Sequenciais Simples\n",
    "\n",
    "Cadeias fÃ¡ceis onde vocÃª pode usar a saÃ­da de um LLM como entrada em outro. Bom para dividir tarefas (e manter seu LLM focado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "llm = OpenAI(temperature=1, openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Seu trabalho Ã© inventar um prato clÃ¡ssico da Ã¡rea que os usuÃ¡rios sugerem.\n",
    "% USER LOCATION\n",
    "{user_location}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"user_location\"], template=template)\n",
    "\n",
    "# MantÃ©m minha cadeia de 'localizaÃ§Ã£o'\n",
    "location_chain = LLMChain(llm=llm, prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Dada uma refeiÃ§Ã£o, dÃª uma receita curta e simples de como fazer esse prato em casa.\n",
    "% MEAL\n",
    "{user_meal}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"user_meal\"], template=template)\n",
    "\n",
    "# MantÃ©m minha cadeia de 'refeiÃ§Ã£o'\n",
    "meal_chain = LLMChain(llm=llm, prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_chain = SimpleSequentialChain(chains=[location_chain, meal_chain], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mO Ravioli alla Carbonara di Roma Ã© um prato tradicional romano. Ã‰ feito com massa de ravioli caseiro e preenchido com molho de bacon, gemas de ovo, parmesÃ£o e pimenta. O prato Ã© um prato tÃ­pico do Lazio, mas Ã© extremamente popular em toda a ItÃ¡lia.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m\n",
      "Ravioli alla Carbonara di Roma\n",
      "\n",
      "Ingredientes:\n",
      "- 500g de farinha de trigo\n",
      "- 4 ovos\n",
      "- 200g de bacon picado\n",
      "- 200g de queijo parmesÃ£o ralado\n",
      "- Sal e pimenta a gosto\n",
      "\n",
      "Instructions:\n",
      "\n",
      "1. Misture a farinha e os ovos atÃ© obter uma massa homogÃªnea.\n",
      "2. Coloque a massa em uma superfÃ­cie lisa e abra-a com um rolo atÃ© ficar com cerca de 3mm de espessura.\n",
      "3. Corte cÃ­rculos de 8-10 cm usando um cortador de biscoitos.\n",
      "4. Coloque um pouco de bacon picado em cada cÃ­rculo e dobre em dois.\n",
      "5. Pressione o bordo da massa com os dedos para fechar os ravioli.\n",
      "6. Cozinhe os ravioli em Ã¡gua fervente por cerca de 4-5 minutos.\n",
      "7. Escorra\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "review = overall_chain.run(\"Roma\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Cadeia de resumo\n",
    "\n",
    "Percorra facilmente vÃ¡rios documentos longos e obtenha um resumo. Confira [este vÃ­deo](https://www.youtube.com/watch?v=f9_BWhCI4Zo) para outros tipos de correntes alÃ©m de map-reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"January 2017Because biographies of famous scientists tend to \n",
      "edit out their mistakes, we underestimate the \n",
      "degree of risk they were willing to take.\n",
      "And because anything a famous scientist did that\n",
      "wasn't a mistake has probably now become the\n",
      "conventional wisdom, those choices don't\n",
      "seem risky either.Biographies of Newton, for example, understandably focus\n",
      "more on physics than alchemy or theology.\n",
      "The impression we get is that his unerring judgment\n",
      "led him straight to truths no one else had noticed.\n",
      "How to explain all the time he spent on alchemy\n",
      "and theology?  Well, smart people are often kind of\n",
      "crazy.But maybe there is a simpler explanation. Maybe\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"the smartness and the craziness were not as separate\n",
      "as we think. Physics seems to us a promising thing\n",
      "to work on, and alchemy and theology obvious wastes\n",
      "of time. But that's because we know how things\n",
      "turned out. In Newton's day the three problems \n",
      "seemed roughly equally promising. No one knew yet\n",
      "what the payoff would be for inventing what we\n",
      "now call physics; if they had, more people would \n",
      "have been working on it. And alchemy and theology\n",
      "were still then in the category Marc Andreessen would \n",
      "describe as \"huge, if true.\"Newton made three bets. One of them worked. But \n",
      "they were all risky.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\" Biographies of famous scientists tend to highlight their accomplishments while excluding mistakes, leading people to overlook the risks they took. Newton's experiments on alchemy and theology often go ignored or underestimated, making his choices look like unerring judgment when in fact, they could have been risky. Maybe there is an alternate explanation.\n",
      "\n",
      " Newton made three betsâ€”physics, alchemy and theologyâ€”and only one ultimately paid off. However, all three seemed to be equally promising in Newton's time since the payout of inventing what is now physics was still unknown. Therefore, the \"smartness\" and \"craziness\" of his bets were intermixed.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Biographies of famous scientists frequently ignore or downplay their risks by emphasizing solely their achievements, leaving the impression that their decisions were error-free. According to this article, Isaac Newton was apparently quite daring; although only one of his bets--physics--ultimately paid off, all three were of equal value at the time he took them since the merit of physics was not yet known. His choices were a combination of \"smartness\" and \"craziness\", reflecting the risk inherent in any innovator\\'s venture.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = TextLoader('data/PaulGrahamEssays/disc.txt')\n",
    "documents = loader.load()\n",
    "\n",
    "# Prepare seu divisor\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=50)\n",
    "\n",
    "# Divida seus documentos em textos\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# HÃ¡ muita complexidade escondida nesta linha. Eu encorajo vocÃª a conferir o vÃ­deo acima para mais detalhes\n",
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\", verbose=True)\n",
    "chain.run(texts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agentes ðŸ¤–ðŸ¤–\n",
    "\n",
    "A documentaÃ§Ã£o oficial da LangChain descreve os agentes perfeitamente (Ãªnfase minha):\n",
    "> Alguns aplicativos exigirÃ£o nÃ£o apenas uma cadeia predeterminada de chamadas para LLMs/outras ferramentas, mas potencialmente uma **cadeia desconhecida** que depende da entrada do usuÃ¡rio. Nesses tipos de cadeias, existe um â€œagenteâ€ que tem acesso a um conjunto de ferramentas. Dependendo da entrada do usuÃ¡rio, o agente pode entÃ£o **decidir qual dessas ferramentas chamar**.\n",
    "\n",
    "\n",
    "Basicamente, vocÃª usa o LLM nÃ£o apenas para saÃ­da de texto, mas tambÃ©m para tomada de decisÃ£o. A frieza e o poder dessa funcionalidade nÃ£o podem ser exagerados o suficiente.\n",
    "\n",
    "Sam Altman enfatiza que os LLMs sÃ£o bons '[mecanismo de raciocÃ­nio](https://www.youtube.com/watch?v=L_Guz73e6fw&t=867s)'. Agente tire proveito disso."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agentes\n",
    "\n",
    "O modelo de linguagem que impulsiona a tomada de decisÃ£o.\n",
    "\n",
    "Mais especificamente, um agente recebe uma entrada e retorna uma resposta correspondente a uma aÃ§Ã£o a ser executada junto com uma entrada de aÃ§Ã£o. VocÃª pode ver diferentes tipos de agentes (que sÃ£o melhores para diferentes casos de uso) [aqui](https://python.langchain.com/en/latest/modules/agents/agents/agent_types.html)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ferramentas\n",
    "\n",
    "Uma 'capacidade' de um agente. Esta Ã© uma abstraÃ§Ã£o em cima de uma funÃ§Ã£o que facilita a interaÃ§Ã£o dos LLMs (e agentes) com ela. Exemplo: pesquisa no Google.\n",
    "\n",
    "Esta Ã¡rea compartilha pontos em comum com [plug-ins OpenAI](https://platform.openai.com/docs/plugins/introduction)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conjunto de ferramentas\n",
    "\n",
    "Grupos de ferramentas que seu agente pode selecionar\n",
    "\n",
    "Vamos juntÃ¡-los:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.llms import OpenAI\n",
    "import json\n",
    "\n",
    "llm = OpenAI(temperature=0, openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "serpapi_api_key='5facac9be1461ad40835cb370aed6c901b4361a81a7dd4ef940ac6f6e6ace6c0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkit = load_tools([\"serpapi\"], llm=llm, serpapi_api_key=serpapi_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = initialize_agent(toolkit, llm, agent=\"zero-shot-react-description\", verbose=True, return_intermediate_steps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should research the band and Natalie Bergman\n",
      "Action: Search\n",
      "Action Input: Natalie Bergman band\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mNatalie Bergman is an American singer-songwriter. She is one half of the duo Wild Belle, along with her brother Elliot Bergman. Her debut solo album, Mercy, was released on Third Man Records on May 7, 2021. She is based in Los Angeles.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should look for the name of the first album\n",
      "Action: Search\n",
      "Action Input: Wild Belle first album\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mIsles\u001b[0m\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: Isles\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response = agent({\"input\":\"qual foi o primeiro Ã¡lbum da\"\n",
    "                    \"banda da qual Natalie Bergman faz parte?\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "el_teste",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
